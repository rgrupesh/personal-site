[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I am a senior pursuing Bachelor of Engineering in Computer Engineering at Nepal Engineering College. My core focus and experience lie in the areas of Machine Learning and Deep Learning (especially Natural Language Processing), and I would like to work around these domains in the future.\n\nExperience:\n\nData Science Intern | FutureSmart AI | Jan 2023 - Present\nLFX Mentee | The Linux Foundation | June 2022 - Aug. 2022\nMachine Learning Intern | Prodigal AI Technologies Pvt. Ltd. | Feb. 2022 - Apr. 2022\nML Project Intern | Indian Institute of Technology, Indore | Jul. 2021 - Oct. 2021\nOpen Source Contributor | GitHub | Organizations: Ivy, Karmada, PolyPhy, Internet Archive, UNICEF, OCaml, Mboalab| 2019 - Present"
  },
  {
    "objectID": "posts/full-stack/Full-Stack-JavaScipt.html",
    "href": "posts/full-stack/Full-Stack-JavaScipt.html",
    "title": "Full Stack Web Development with JavaScript",
    "section": "",
    "text": "JavaScript has come long way in the last 20 years. JavaScript engine were orginally used only in web browsers, but they are now embedded even in some servers, usually via Node.js. This evolution has made JavaScript a universal language.\n\nWhai is Full Stack Web Development?\n\nFull Stack is the development of both front end/client-side and back end/server-side. Full Stack Web Development can be divided into three major layers: presentation layer, business logic layer, and data access layer. Presentation layer handles UI/UX, business logic layer deals with dyanamic data processing, and data access layer provides data acesss through an API.\n\n\nHow to Become a Full Stack Web Developer?\n\nFirstly, developer should have knowledge of front end technologies like BootStrap, Javascript, React, Angular or Vue in addition to HTML and CSS. Secondly, they should be equally good with backend tools like Node.js, Django, Express and language like Python, ASP.NET, Java, C++, PHP. Finally,they are also required to have knowledge of databases like SQL, MYSQL, SQLite or MongoDB.\n\nFurther, a Full Stack developer can specialize into one or many software stack.For instance:\n1: MEAN Stack: MONGO | EXPRESS | ANGULAR| NODE\n2: MERN Stack: MONGO | EXPRESS | REACT| NODE\n3: MEVN Stack: MONGO | EXPRESS | VUE | NODE\n4: LAMP Stack: LINUX | APACHE | MYSQL | PHP\n5: Django Stack: JAVASCRIPT | PYTHON | DJANGO | MYSQL\n\nJavascript Full Stack\n\nLearning various languages can be cumbersome at times. This is where JavaScript comes in. In JavaScript Full Stack, presentaion layer is built using JavaScript frameworks, such as: React, Angular or Vue. Similarly, business logic layer is constructed with Node.js and node-modules to communicate with front end using REST API serving JSON. Finally, MongoDB is used in data access layer.\n\nThere’s trade-off everywhere. JavaScript Full Stack Web Dev is no exception.\nPros - Prototype can be build rapidly. - Common language, better team efficiency with less resources. - Code reusability. - Can switch between front and back end development based on requirements.\nCons - Node.js is not recommended for heavy backend computation and data processing. - Jack of all trades, master of none? May be."
  },
  {
    "objectID": "posts/transformer/Attention-Is-All-You-Need.html",
    "href": "posts/transformer/Attention-Is-All-You-Need.html",
    "title": "Attention Is All You Need",
    "section": "",
    "text": "Attention Is All You Need is a breakthrough Natural Language Processing(NLP) paper published by the Google Research team in 2017. The paper proposed a transformer architecture consisting of six encoder and decoder blocks stacked on top of each other. Since then it has taken the NLP world by storm. They are used in many applications like machine language translation, conversational chatbots, and even to power better search engines. These days we see this architecture used in computer vision as well.\nLet’s begin by looking at the model as a single black box where we need to perform language translation(say English to Nepali). What is inside the black box? Let’s find out.\n\nThere are encoder and decoder modules in the transformer. The encoding component is a stack of six encoders and the decoding component is the same but with a decoder. Each encoder is connected to all six decoders.\nThe encoder itself consists of two layers: self-attention and feed-forward layer. The self-attention layer of the first encoder takes embedded input. Each input word is embedded into a vector of size 512, before passing that to the transformer. While giving input to the model along with the embedding vector, the positional vector is added to account for the order of the words in the input sequence. After that, the result of the first self-attention is passed to the first feed-forward neural network within the first encoder block. The output of the first feed-forward layer is passed to the second encoder and so on.\n\nThe self-attention layer deals with the subtlety of the language. Let’s dive deeper into the self-attention layer.\nThe first step is to create three addition vectors named as key, query, and value. These vectors are just the abstraction to calculate the self-attention score. After that, we calculate the score by taking the dot product of query1key1, query1key2, and so on. We have a key vector of all input words since inputs are given parallelly. Also, we divide each score by sqrt of the dimension of a key vector which is 8, as the dimension of a key vector discussed in the paper is 64. Further, the result is passed through softmax operation so all the scores are positive and add up to 1. The output of softmax is multiplied with the value vector(v1). Then, the result is summed up to produce the self-attention output of the first word( Say Z1). This Z1 is sent to feed-forward network. This process is repeated for every input word.\n\nTo further catch the subtlety of the language, the paper improves the self-attention layer by adding a mechanism called “multi-headed” attention. It is the same as self-attention but with multiple keys, queries, and values matrix. The paper uses 8 attention heads. So, we will get 8 different z1 matrices for each input word. We concat all of them. During training, we initialize another matrix(W0) which we will use to multiply our concatenated matrix to get a single Z1.\nThe decoder is similar to that of the encoder but with one additional layer. The new layer, the encoder-decoder attention layer, is sandwiched between self-attention and feed-forward neural network. E-D layer creates queries matrix from the layer below it and takes keys and values from the output of the encoder and does the same calculation as multi-headed self-attention. The self-attention layer in the decoder is only allowed to know the earlier position of the output sequence. This is done by masking future positions before the softmax step.\nLikewise, layers are normalized after every self-attention and feed-forward layer. Also, each encoder and decoder has a residual connection around it.\n\nLastly, the output of the decoder is passed through the linear and softmax layer. The linear layer is a simply connected neural network. It is as wide as the vocab size of our model. Each cell here corresponds to unique words. This layer is passed to the softmax, which turns all the scores into probability. And, the word with the highest probability is chosen."
  },
  {
    "objectID": "posts/building-NLP-model/NLP.html",
    "href": "posts/building-NLP-model/NLP.html",
    "title": "Building NLP model",
    "section": "",
    "text": "With the availability huge amount of text data, NLP(Natural Language Processing) has been gaining popularity in recent years. In this article, I will be explaining the necessary steps to develop NLP model. The development of the NLP model can be broadly classified into three categories: preprocessing data, generating embedding, and building model.\nLet’s dive into these!"
  },
  {
    "objectID": "posts/building-NLP-model/NLP.html#data-preprocessing",
    "href": "posts/building-NLP-model/NLP.html#data-preprocessing",
    "title": "Building NLP model",
    "section": "1. Data Preprocessing",
    "text": "1. Data Preprocessing\nData preprocessing is an initial and crucial step in machine learning. It is the process of transforming raw data into a useful and efficient format. Some of the tasks include removing special characters, website links, stop words, performing tokenization, normalization.\n\nCleaning\nIn this step, we remove website links, special characters, numbers, emojis. We only want text data.\n\n\nTokenization\nTokenization means converting your text/sentences into small units. For instance:\n\nsentence\n\n\njk rowling wishes snape happy birthday magical way\n\n\ntokenized sentence\n\n\n['jk', 'rowling', 'wishes', 'snape', 'happy', 'birthday', 'magical', 'way']]\n\n\n\nNormalization\nNormalization is the process of converting a word to its root form. For instance: ‘plays’, ‘played’, ‘playing’ all essentially means to play, so we convert these words to ‘play’. It is an essential part of feature engineering where we convert higher dimension features to low dimensional space.\n\nstemming\n\nIt is the process of converting word to its non changing form.\n\nprocessing-->process\nnatural-->natur\n\n\nlemmatization\n\nIt is the process of converting word to its dictionary form.\n\nbetter-->good\nwomen-->woman\n\n\n\nStopwords\nStopwords do not add much value to our NLP model. So, we remove all of them. This helps us to make our model more robust.\nYou can check the list of English stopwords here."
  },
  {
    "objectID": "posts/building-NLP-model/NLP.html#building-embedding",
    "href": "posts/building-NLP-model/NLP.html#building-embedding",
    "title": "Building NLP model",
    "section": "2. Building Embedding",
    "text": "2. Building Embedding\nWord embedding is a representation of words where similar words have similar representations in vector space. Here, words are represented as real-valued vectors.\n\nBOW\nBag of Words(BOW) is a technique to convert a word into a vector, based on the frequency of the word. It creates a vocabulary set and captures the distribution of words in each document.\n\n\n\nTF-IDF\nTerm Frequency - Inverse Document Frequency(TF-IDF) is a method of representing word vectors where the words that are unique to each document(instance of corpus) are given higher importance than common occurring words. It is calculated by:\n\nTF-IDF = TF * IDF\nwhere, TF = probability of a word in a specific document\nIDF = probability of that word in overall corpus\n\n\n\nWord2Vec\nWord2Vec is a pre-trained model developed by Google which was trained on 300 million words of news corpus. It was trained using both the CBOW(Continous Bag of Words) and Skip-gram technique. In CBOW, the target word is predicted using context word and in skip-gram, we use target word to predict context word. Word2Vec represents each word in 300-dimensional vector space which helps to catch the subtle relationship between words. Paper\n\n\n\nGloVe\nGloVe stands for global vectors. It is an embedding technique that uses a co-occurrence(how frequent words appear together) matrix to build word vectors at the global level. Using this sort of matrix effectively captures the meaning of the words but the size of that matrix becomes huge. The GloVe paper addresses this issue by factorizing that matrix to lower dimension. Paper"
  },
  {
    "objectID": "posts/building-NLP-model/NLP.html#building-model",
    "href": "posts/building-NLP-model/NLP.html#building-model",
    "title": "Building NLP model",
    "section": "3. Building Model",
    "text": "3. Building Model\nOnce we have a numeric representation of our data, we should look to build the model. There are many options to choose from. We should always choose according to our problem statement. Some algorithm works well on one set of a problem but poor on other.\n\nTraditional Machine Learning algorithms\nThe traditional machine learning algorithm like Naive Bayes, Random Forest, Support Vector Machine, XGBoost can also be used in NLP tasks.\n\n\nRecurrent neural network(RNN)\nRecurrent Neural Networks work well with text or sequential data. The vanilla RNN or the advanced form of RNN like LSTM and GRU can be used to train our NLP model. The latter prevents issues like vanishing and exploding gradient.\nGRU\nGated Recurrent Unit(GRU) has two gates that are reset gate and update gate. GRU is relatively new and in some cases, performance is as good as LSTM even having such a simple architecture. \nLSTM\nLong Short Term Memory(LSTM) has three gates that are input, output, and forget gate. LSTM is still preferred if we need to train our model for longer sequences.\n\n\n\nTransformers\nTransformers architecture uses attention mechanism to handle long-range dependencies. This architecture was published in the paper Attention Is All You Need.\nAlso, I have written whole about this architecture here."
  },
  {
    "objectID": "posts/mobile-development/mob-dev-framework.html",
    "href": "posts/mobile-development/mob-dev-framework.html",
    "title": "Choosing the Best Mobile App Framework",
    "section": "",
    "text": "Choosing a mobile develpoment framework can be daunting at times.There are many option and consideration to make outside the framework.\nConsideration: - Time to Market - Development Cost - Maintaince Cost - App Performance - Feature Availabilty - 3rd Party Risk\n\n1st Party Native App:\n\nGoogle Android and Apple iOS are leading mobile operating system. Android is the leading mobile OS market with almost 75 percent share. Both Android and iOS jointly possess almost 99 percent of global market share.\n\nApple makes Xcode development toolkit that allows us to develop iOS app using Objective-C and Swift.Similary, Android App can be developed using Android Studio using Java and Kotlin.Merits of using 1st Party Native includes:\n\nNative UI/UX\nNew feature or API right after release\n\n\nProgressive Web App\n\nProgressive Web App is an app built from the web technologies like HTML, CSS, and JavaScript, but with a feel and functionality that rivals an actual native app.Frameworks includes: Angularjs, React.\n\nDemerits - Slower than native app - UI/UX limitation\nMerits - Cheaper to develop,deploy - Require less storage - No store required\n\nCross-Platform Native Apps\n\nCross-Platform Native Apps are created using Xamarin,React Native,Flutter. These frameworks not only allows us to create totally native application that gives us access to every single iOS and Android API but also allows us to write the app in same code base so that we can share code between iOS and Android.\n\n\nMicrosoft\n\nXamarin\nLanguage: .NET, C#, F#\n\nFacebook\n\nReact Native\n\nLanguage: JavaScript\n\nGoogle\n\nFlutter\nLanguage: DART"
  },
  {
    "objectID": "posts/numpy-pandas/index.html",
    "href": "posts/numpy-pandas/index.html",
    "title": "NumPy and Pandas",
    "section": "",
    "text": "The NumPy library is the core library for scientific computing in Python. It provides a high-performance multidimensional array object, and tools for working with arrays. Whereas, Pandas library is built on NumPy and provides easy-to-use data structures and data analysis tools for the Python programming language.\n\nInstalling\n\nNumpy - pip install numpy\nPandas - pip install pandas\nNumPy:\n- creating array\n    - a = np.array([1,2,3])\n- Create an array of zeros\n    - np.zeros((3,4))\n- Create an array with random values\n    - np.random.random((2,2))\n- Create an empty array\n    - np.empty((3,2))\n- Transposing Array\n   - i = np.transpose(b)\n- Append items to an array\n   - np.append(h,g)\n- Delete items from an array\n   - np.delete(a,[1])\nPandas:\n- Series\n  - s = pd.Series([3, -5, 7, 4], index=['a', 'b', 'c', 'd'])\n- DataFrame\n   - data = {'Country': ['Belgium', 'India', 'Brazil'], 'Capital': ['Brussels', 'New Delhi', 'Brasília'],'Population': [11190846, 1303171035, 207847528]}\n   - df = pd.DataFrame(data,columns=['Country', 'Capital', 'Population'])\n- Read and Write to CSV\n   - pd.read_csv('file.csv', header=None, nrows=5)\n   - df.to_csv('myDataFrame.csv')\n- Read and Write to Excel\n   - pd.read_excel('file.xlsx')\n   - pd.to_excel('dir/myDataFrame.xlsx', sheet_name='Sheet1')\n- Info on DataFrame\n  - df.info()"
  },
  {
    "objectID": "posts/BERT-vs-XLNET/BERT-vs-XLNet.html",
    "href": "posts/BERT-vs-XLNET/BERT-vs-XLNet.html",
    "title": "BERT vs. XLNet",
    "section": "",
    "text": "Transformer based model has been key to recent advancement in the field of Natural Language Processing. The reason behind this success is technique called Transfer Learning. Although computer vision practitioners are well-versed with this technique, it is relatively new to the field of NLP. In Transfer Learning, a model (in our case, a Transformer model) is pre-trained on a huge dataset using an unsupervised pre-training objective. This same model is then fine-tuned on the actual task. This approach works exceptionally well, even if you have as small as 500–1000 training samples.\nFurther, two pretraining objectives that have been successful for pretraining neural networks used in transfer learning NLP are autoregressive (AR) language modeling and autoencoding (AE). The AR model can only use forward context or backward context at a time whereas AE language model can do both at a same time. BERT is AE whereas GPT is AR language model.\n\nBERT\n\nBERT(Bidirectional Encoder Representations from Transformers ) as its name suggests is a bidirectional autoencoder(AE) language model. It obtained state-of-the-art results on 11 Natural Language Processing tasks when it was published.\n\nHow does BERT work?\n\n\nArchitecture\n\nBERT currently has two variant.\n\nBERT Base: 12 layers, 12 attention heads, and 110 million parameters\nBERT Large: 24 layers, 16 attention heads, and 340 million parameters\n\n\n\nProcessing and Pre-training\n\nBERT undergoes three layers of abstraction to preserve true meaning of input text.\n\nBERT is pre-trained on two NLP tasks:\n\nMasked Language Modelling : In a broad sense, it replaces word with [MASK] token and trains in such a way that model will be able to predict missing word.\nNext Sentence Prediction : Here, given two sentences – A and B, model is asked to predict, is B the actual next sentence that comes after A in the corpus, or just a random sentence?\n\nLastly, we fine tune this pre-trained model to perform specific NLP task.\n\n\nXLNet\n\nXLNet is a “generalized” autoregressive(AR) language model that enables learning bidirectional contexts using Permutation Language Modeling. XLNet borrows the ideas from both AE and AR language model while avoiding their limitation. As per paper, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.\n\nHow does XLNet work?\n\n\nArchitecture\n\nSame as BERT, XLNet currently has two variant.\n\nXLNet-Base, cased: 12 layers, 12 attention heads, and 110 million parameters\nXLNet-Large, cased: 24 layers, 16 attention heads, and 340 million parameters\n\n\nProcessing and Pre-training\n\nPermutation Language Modeling(PLM) is the concept of training bi-directional AR model on all permutation of words in a sentence. XLNet makes use of PLM to achieve state-of-the-art(SOTA) results. Besides, XLNet is based on the Transformer-XL which it uses as the main pretraining framework. It adopts the method like segment recurrent mechanism and relative encoding from Transformer-XL model.\n\n\nWhich one should you choose?\n\nBoth BERT and XLNet are impressive language model. I recommened you to start with BERT and Transformer-XL then get into XLNet.\n\n\n\nPapers\n\n\nXLNet: Generalized Autoregressive Pretraining for Language Understanding\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to my Blog!",
    "section": "",
    "text": "Building NLP model\n\n\n\n\n\n\n\nRNN\n\n\nLSTM\n\n\nGRU\n\n\nNLP\n\n\nSOTA\n\n\nword2vec\n\n\nGloVe\n\n\nNLTK\n\n\nregex\n\n\nTransformers\n\n\nembeddings\n\n\nDeep Learning\n\n\n\n\n\n\n\n\n\n\n\nOct 4, 2021\n\n\nRupesh Gelal\n\n\n\n\n\n\n  \n\n\n\n\nAttention Is All You Need\n\n\n\n\n\n\n\nTransfomer\n\n\nEncoder\n\n\nDecoder\n\n\nNLP\n\n\nBERT\n\n\nGPT\n\n\nSOTA\n\n\nDeep Learning\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 2021\n\n\nRupesh Gelal\n\n\n\n\n\n\n  \n\n\n\n\nBERT vs. XLNet\n\n\n\n\n\n\n\nDeep Learning\n\n\nNLP\n\n\nTransformers\n\n\nXLNet\n\n\nBERT\n\n\nTransfer Learning\n\n\nSOTA\n\n\n\n\n\n\n\n\n\n\n\nAug 10, 2020\n\n\nRupesh Gelal\n\n\n\n\n\n\n  \n\n\n\n\nFull Stack Web Development with JavaScript\n\n\n\n\n\n\n\nHTML\n\n\nCSS\n\n\nJavaScript\n\n\nAngular\n\n\nReact\n\n\nVue\n\n\nNode.js\n\n\nExpress\n\n\nMongoDB\n\n\n\n\n\n\n\n\n\n\n\nJul 24, 2020\n\n\nRupesh Gelal\n\n\n\n\n\n\n  \n\n\n\n\nChoosing the Best Mobile App Framework\n\n\n\n\n\n\n\niOS\n\n\nAndroid\n\n\nJavaScript\n\n\n\n\n\n\n\n\n\n\n\nJun 8, 2020\n\n\nRupesh Gelal\n\n\n\n\n\n\n  \n\n\n\n\nNumPy and Pandas\n\n\n\n\n\n\n\nPython\n\n\nNumPy\n\n\nPandas\n\n\n\n\n\n\n\n\n\n\n\nMay 1, 2020\n\n\nRupesh Gelal\n\n\n\n\n\n\nNo matching items"
  }
]