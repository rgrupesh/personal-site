{"title":"BERT vs. XLNet","markdown":{"yaml":{"title":"BERT vs. XLNet","image":"language.jpg","author":"Rupesh Gelal","date":"2021-01-01","categories":["NLP","Transformers","XLNet","BERT","Transfer Learning"]},"containsRefs":false,"markdown":"\n\n[Transformer](https://arxiv.org/pdf/1706.03762.pdf) based model has been key to recent advancement in the field of Natural Language Processing. The reason behind this success is technique called [Transfer Learning](<https://en.wikipedia.org/wiki/Transfer_learning#:~:text=Transfer%20learning%20(TL)%20is%20a,when%20trying%20to%20recognize%20trucks.>). Although computer vision practitioners are well-versed with this technique, it is relatively new to the field of NLP. In Transfer Learning, a model (in our case, a Transformer model) is pre-trained on a huge dataset using an unsupervised pre-training objective. This same model is then fine-tuned on the actual task. This approach works exceptionally well, even if you have as small as 500–1000 training samples.\n\nFurther, two pretraining objectives that have been successful for pretraining neural networks used in transfer learning NLP are autoregressive (AR) language modeling and autoencoding (AE). The AR model can only use forward context or backward context at a time whereas AE language model can do both at a same time. BERT is AE whereas GPT is AR language model.\n\n<h2>BERT</h2>\n\n[BERT](https://arxiv.org/pdf/1810.04805.pdf)(Bidirectional Encoder Representations from Transformers ) as its name suggests is a bidirectional autoencoder(AE) language model. It obtained state-of-the-art results on 11 Natural Language Processing tasks when it was published.\n\n<h3>How does BERT work?</h3>\n\n<h4>Architecture</h4>\n\nBERT currently has two variant.\n\n- BERT Base: 12 layers, 12 attention heads, and 110 million parameters\n- BERT Large: 24 layers, 16 attention heads, and 340 million parameters\n\n![](archi.png)\n\n<h4>Processing and Pre-training</h4>\n\nBERT undergoes three layers of abstraction to preserve true meaning of input text.\n\n![](embedding.png)\n\nBERT is pre-trained on two NLP tasks:\n\n- Masked Language Modelling : In a broad sense, it replaces word with [MASK] token and trains in such a way that model will be able to predict missing word.\n\n- Next Sentence Prediction : Here, given two sentences – A and B, model is asked to predict, is B the actual next sentence that comes after A in the corpus, or just a random sentence?\n\nLastly, we fine tune this pre-trained model to perform specific NLP task.\n\n![](bert.png)\n\n<h2>XLNet</h2>\n\nXLNet is a \"generalized\" autoregressive(AR) language model that enables learning bidirectional contexts using [Permutation Language Modeling](https://arxiv.org/pdf/1906.08237.pdf). XLNet borrows the ideas from both AE and AR language model while avoiding their limitation. As per paper, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.\n\n<h3>How does XLNet work?</h3>\n\n<h4>Architecture</h4>\n\nSame as BERT, XLNet currently has two variant.\n\n- XLNet-Base, cased: 12 layers, 12 attention heads, and 110 million parameters\n- XLNet-Large, cased: 24 layers, 16 attention heads, and 340 million parameters\n\n<h4>Processing and Pre-training</h4>\n\nPermutation Language Modeling(PLM) is the concept of training bi-directional AR model on all permutation of words in a sentence. XLNet makes use of PLM to achieve state-of-the-art(SOTA) results. Besides, XLNet is based on the [Transformer-XL](https://arxiv.org/pdf/1901.02860.pdf) which it uses as the main pretraining framework. It adopts the method like segment recurrent mechanism and relative encoding from Transformer-XL model.\n\n![](xlnet.png)\n\n<h2>Which one should you choose?</h2>\n\nBoth BERT and XLNet are impressive language model. I recommened you to start with BERT and Transformer-XL then get into XLNet.\n\n![](scores.png)\n\n![](comparision.png)\n\n<h2>Papers</h2>\n\n- [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/pdf/1906.08237.pdf)\n- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"BERT-vs-XLNet.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.280","theme":"yeti","title-block-banner":true,"title":"BERT vs. XLNet","image":"language.jpg","author":"Rupesh Gelal","date":"2021-01-01","categories":["NLP","Transformers","XLNet","BERT","Transfer Learning"]},"extensions":{"book":{"multiFile":true}}}}}