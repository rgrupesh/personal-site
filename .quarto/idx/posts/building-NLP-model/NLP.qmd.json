{"title":"Building NLP model","markdown":{"yaml":{"title":"Building NLP model","image":"nlp.jpg","author":"Rupesh Gelal","date":"2022-05-04","categories":["RNN","LSTM","GRU","NLP","word2vec","GloVe","Transformers"]},"headingText":"1. Data Preprocessing","containsRefs":false,"markdown":"\n\nWith the availability huge amount of text data, NLP(Natural Language Processing) has been gaining popularity in recent years. In this article, I will be explaining the necessary steps to develop NLP model.\nThe development of the NLP model can be broadly classified into three categories: preprocessing data, generating embedding, and building model.\n\nLet's dive into these!\n\n\nData preprocessing is an initial and crucial step in machine learning. It is the process of transforming raw data into a useful and efficient format. Some of the tasks include removing special characters, website links, stop words, performing tokenization, normalization.\n\n#### Cleaning\n\nIn this step, we remove website links, special characters, numbers, emojis. We only want text data.\n\n#### Tokenization\n\nTokenization means converting your text/sentences into small units. For instance:\n\n- sentence\n\n>`jk rowling wishes snape happy birthday magical way`\n\n- tokenized sentence\n\n>`['jk', 'rowling', 'wishes', 'snape', 'happy', 'birthday', 'magical', 'way']]`\n\n#### Normalization\n\nNormalization is the process of converting a word to its root form. For instance: 'plays', 'played', 'playing' all essentially means to play, so we convert these words to 'play'. It is an essential part of feature engineering where we convert higher dimension features to low dimensional space. \n\n- ##### stemming\n\nIt is the process of converting word to its non changing form.\n\n>`processing-->process`\n>\n>`natural-->natur`\n\n- ##### lemmatization\n\nIt is the process of converting word to its dictionary form.\n\n>`better-->good`\n>\n>`women-->woman`\n\n#### Stopwords\n\nStopwords do not add much value to our NLP model. So, we remove all of them. This helps us to make our model more robust.\n\nYou can check the list of English stopwords [here.](https://gist.github.com/sebleier/554280)\n\n## 2. Building Embedding\n\nWord embedding is a representation of words where similar words have similar representations in vector space. Here, words are represented as real-valued vectors.\n\n#### BOW\n\nBag of Words(BOW) is a technique to convert a word into a vector, based on the frequency of the word. It creates a vocabulary set and captures the distribution of words in each document.\n\n![](bow.png)\n\n#### TF-IDF\n\nTerm Frequency - Inverse Document Frequency(TF-IDF) is a method of representing word vectors where the words that are unique to each document(instance of corpus) are given higher importance than common occurring words. It is calculated by:\n\n>`TF-IDF = TF * IDF`\n>\n>`where, TF = probability of a word in a specific document`\n>\n>`IDF = probability of that word in overall corpus`\n\n#### Word2Vec\n\nWord2Vec is a pre-trained model developed by Google which was trained on 300 million words of news corpus. It was trained using both the CBOW(Continous Bag of Words) and Skip-gram technique. In CBOW, the target word is predicted using context word and in skip-gram, we use target word to predict context word. Word2Vec represents each word in 300-dimensional vector space which helps to catch the subtle relationship between words. \n[Paper](https://arxiv.org/abs/1301.3781)\n\n![](word2vec.png)\n\n#### GloVe\n\nGloVe stands for global vectors. It is an embedding technique that uses a co-occurrence(how frequent words appear together) matrix to build word vectors at the global level. Using this sort of matrix effectively captures the meaning of the words but the size of that matrix becomes huge. The GloVe paper addresses this issue by factorizing that matrix to lower dimension.\n[Paper](https://nlp.stanford.edu/pubs/glove.pdf)\n\n\n## 3. Building Model\n\nOnce we have a numeric representation of our data, we should look to build the model. There are many options to choose from. We should always choose according to our problem statement. Some algorithm works well on one set of a problem but poor on other.\n\n#### Traditional Machine Learning algorithms\n\nThe traditional machine learning algorithm like Naive Bayes, Random Forest, Support Vector Machine, XGBoost can also be used in NLP tasks.\n\n#### Recurrent neural network(RNN)\n\nRecurrent Neural Networks work well with text or sequential data. The vanilla RNN or the advanced form of RNN like LSTM and GRU can be used to train our NLP model. The latter prevents issues like vanishing and exploding gradient.\n\n`GRU`\n\nGated Recurrent Unit(GRU) has two gates that are reset gate and update gate. GRU is relatively new and in some cases, performance is as good as LSTM even having such a simple architecture.\n![](GRU.png)\n\n`LSTM`\n\nLong Short Term Memory(LSTM) has three gates that are input, output, and forget gate. LSTM is still preferred if we need to train our model for longer sequences.\n\n![](LSTM.png)\n\n#### Transformers\n\nTransformers architecture uses attention mechanism to handle long-range dependencies. This architecture was published in the paper [Attention Is All You Need.](https://arxiv.org/pdf/1706.03762.pdf)\n\nAlso, I have written whole about this architecture [here.](https://rupeshgelal.com.np/posts/transformer/attention-is-all-you-need)\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"NLP.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.280","theme":"yeti","title-block-banner":true,"title":"Building NLP model","image":"nlp.jpg","author":"Rupesh Gelal","date":"2022-05-04","categories":["RNN","LSTM","GRU","NLP","word2vec","GloVe","Transformers"]},"extensions":{"book":{"multiFile":true}}}}}